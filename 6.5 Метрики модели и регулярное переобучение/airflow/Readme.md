# Работа с Apache Airflow

## Запуск Airflow
Airflow поднимем в докере

Для этого будем использовать docker-compose.yml от Apache ([подробное описание](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html))

⚠️ Обратите внимание что Apache не рекомендует использовать Airflow в докере для разворачивания в продакшене, вместо этого советуют использовать Kubernetes и Helm Chart от официального Airflow сообщества.

Дополнительно пробросим в контейнеры:
- [папку со скриптами пайплайна](./docker-compose.yml#L82)
- [requirements.txt](./docker-compose.yml#L84)
- [credentials для S3](./docker-compose.yml#L86)

По умолчанию в Airflow будет 74 пайплайна с примерами, если они не нужны, то необходимо переменную окружения [AIRFLOW__CORE__LOAD_EXAMPLES](./docker-compose.yml#L62) установить в значение `'false'`

Поднимем все необходимые сервисы - в папке с `docker-compose.yml` выполним команду (процесс может занять некоторое время)
```shell
docker compose up
```

Если хотите удалить все контейнеры и `volume` БД, то после остановки контейнеров можно выполнить
```shell
# Удалить только контейнеры
docker compose down

# Удалить контейнеры и volume БД
docker compose down --volumes
```

Airflow по умолчанию будет развернут на `8080` порту с логином `airflow` и паролем `airflow`

## Создание пайплайна в Airflow

Для того, чтобы наш пайплайн появился в Airflow, его необходимо положить в [папку](docker-compose.yml#L76) `dags`. Airflow периодически проверяет её и добавляет новые пайплайны

Пайпалайн можно создать в виде python скрипта

[Пример с пайплайном](./dags/crypto_sentiment_dag.py), который мы запускали в DVC

Для удобства запуска пайплайнов разных проектов с разными зависимостями будем в начале пайплайна создавать новое виртуальное окружение и использовать его, а в конце, если есть необходимость - удалять

Для создания виртуального окружения нам надо:
- [функция](./dags/utils.py#L7), создающая окружение
- [функция-обертка](./dags/crypto_sentiment_dag.py#L18) для запуска в пайплайне airflow

Для удаления виртуального окружения нам надо:
- [функция](./dags/utils.py#L29), удаляющая папку с окружением
- [функция-обертка](./dags/crypto_sentiment_dag.py#L27) для запуска в пайплайне airflow

Для создания пайплайна надо:
- определить [переменные по умолчанию](./dags/crypto_sentiment_dag.py#L33)
- создать [DAG](./dags/crypto_sentiment_dag.py#L40) (Directed Acyclic Graph)
- создать [этапы пайплайна](./dags/crypto_sentiment_dag.py#L49)
- определить [последовательность этапов](./dags/crypto_sentiment_dag.py#L132) в пайплайне

⚠️ Обратите внимание что, если для аргумента [start_date](./dags/crypto_sentiment_dag.py#L35) поставить сильно устаревшую дату, то при загрузке пайплайна airflow попытается выполнить все пропущенные запуски

⚠️ Обратите внимения что для `start_date` надо указывать время, которое стоит внутри контейнеров Airflow - по умолчанию это `UTC+0`

**Расписание пайплайна**

Если мы хоти запускать наш пайплайн по расписанию, то необходимо задать значение для аргумента [schedule_interval](./dags/crypto_sentiment_dag.py#L44). 

Возможные значения: `@once`, `@hourly`, `@daily`, `@weekly`, `@monthly`, `@quarterly`, `@yearly` и любое значение в формате cron. Подробнее [здесь](https://airflow.apache.org/docs/apache-airflow/2.2.4/dag-run.html#cron-presets)

Для удобства конвертации интервалов в cron формат можно воспользоваться сайтом [Crontab guru](https://crontab.guru)

**Операторы пайплайна**

Оператор в airflow это строительный блок, который определяет действия, выполняемые на этапах пайплайна

Существует множество разных операторов ([список с подробностями](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html))

Мы будем использовать два: `PythonOperator` - для создания и удаления виртуального окружения и `BashOperator` для запуска наших скриптов

`PythonOperator` принимает на вход любую python функцию (callable)

`BashOperator` принимает на вход команду, которую необходимо выполнить в командной строке

**Сенсоры в Airflow**

В airflow есть сенсоры - это специальный тип операторов, которые ожидают выполнения определенных условий, прежде чем продолжить выполнение этапов в пайплайне (например, появился ли нужный файл, завершен ли какой-то определенный этап пайплайна и т. п.)

Сенсоры позволяют создавать более гибкие пайплайны и использовать триггеры для их запуска

Подробнее можно посмотреть [здесь](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html)

## Airflow Web Server

Сервер Airflow по умолчанию доступен по адресу `<ip_docker_container>:8080`

Логин и пароль - `airflow`

**DAGS**

На вкладке `DAGS` будет список всех доступных пайплайнов в Airflow.

Если по какой-то причине Airflow не смог добавить пайплайн, то вверху страницы появится красное сообщение со всеми ошибками, которые произошли при попытке добавить пайплайн

С помощью переключателя слева от пайплайна его можно поставить на паузу - в таком случае новые запуски не будут создаваться, а текущие продолжат выполняться

Если провалиться в пайплайн, то можно увидеть:
- список этапов пайплайна с диаграммой его запусков
- вкладку `Details` с общим саммари пайплайна
- вкладку `Graph`, где будет отображена последовательность задач в виде графа
- вкладку `Gant`, где можно посмотреть диаграмму Ганта для пайплайна
- вкладку `Code`, где можно посмотреть исходный код нашего пайплайна
- вкладку `Event Log`, где будут отображаться системные события Airflow о работе пайплайна
- вкладку `Calendar`, где можно посмотреть расписание запусков пайплайна
- вкладки `Run Duration` и `Task Duration`, где можно посмотреть время выполнения пайплайна и его этапов
- если провалиться в конкретный этап конкретного запуска (наприме, щелкнув на квадрат с этим этапом в диаграмме слева), то появится вкладка `Logs`, в которой будут логи контейнера с airflow worker - может быть полезно, если надо что-то отдебажить или понять почему ломается пайплайн